# Smart Model Routing Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Implement task-based automatic model selection to achieve 20-30% additional cost savings by routing collection suggestions to gpt-5-nano, next game suggestions to gpt-4o-mini, and cover generation to grok-2-image-1212.

**Architecture:** Create a model router service that maps task types (suggest_collections, suggest_next_game, generate_cover_image) to optimal models with fallback chains. Update AI service to query the router before making API calls. Add database columns and API endpoints for user override preferences.

**Tech Stack:** TypeScript, Drizzle ORM, PostgreSQL, Vercel AI SDK

---

## Task 1: Database Schema for Routing Settings

**Files:**

- Modify: `backend/src/db/schema.ts`
- Migration: Auto-generated by Drizzle

**Step 1: Add routing columns to userAiSettings table**

In `backend/src/db/schema.ts`, add new columns to the `userAiSettings` table definition (around line 450):

```typescript
export const userAiSettings = pgTable("user_ai_settings", {
  // ... existing fields (id, userId, provider, baseUrl, etc.)

  // NEW: Model routing preferences
  collectionSuggestionsModel: varchar("collection_suggestions_model", { length: 100 }),
  nextGameSuggestionsModel: varchar("next_game_suggestions_model", { length: 100 }),
  coverGenerationModel: varchar("cover_generation_model", { length: 100 }),
  enableSmartRouting: boolean("enable_smart_routing").default(true),
});
```

**Step 2: Generate migration**

Run: `cd backend && bun run db:generate`
Expected: Creates new migration file in `drizzle/` directory

**Step 3: Review generated SQL**

Run: `ls -lt drizzle/ | head -5`
Open the newest `.sql` file and verify it contains:

- `ALTER TABLE user_ai_settings ADD COLUMN collection_suggestions_model varchar(100);`
- `ALTER TABLE user_ai_settings ADD COLUMN next_game_suggestions_model varchar(100);`
- `ALTER TABLE user_ai_settings ADD COLUMN cover_generation_model varchar(100);`
- `ALTER TABLE user_ai_settings ADD COLUMN enable_smart_routing boolean DEFAULT true;`

**Step 4: Apply migration**

Run: `bun run db:migrate`
Expected: "Migration applied successfully" or similar output

**Step 5: Verify in database**

Run: `psql postgresql://mymemorycard:devpassword@localhost:5433/mymemorycard -c "\d user_ai_settings"`
Expected: New columns appear in table schema

**Step 6: Verify TypeScript compiles**

Run: `bun run typecheck`
Expected: No errors

**Step 7: Commit**

```bash
git add src/db/schema.ts drizzle/
git commit -m "feat: add model routing settings to user ai settings"
```

---

## Task 2: Model Router Service

**Files:**

- Create: `backend/src/services/ai/model-router.ts`

**Step 1: Create model router file with types**

Create `backend/src/services/ai/model-router.ts`:

```typescript
export type TaskType = "suggest_collections" | "suggest_next_game" | "generate_cover_image";

export interface ModelRoute {
  primary: string;
  fallback: string[];
  maxTokensRecommended: number;
  temperatureRecommended: number | null;
}

export interface ModelSelection {
  model: string;
  maxTokens: number;
  temperature: number | null;
  isReasoningModel: boolean;
}

export interface AiSettings {
  provider: string;
  baseUrl: string | null;
  apiKeyEncrypted: string | null;
  model: string;
  temperature: number;
  maxTokens: number;
  enabled: boolean;
  collectionSuggestionsModel?: string | null;
  nextGameSuggestionsModel?: string | null;
  coverGenerationModel?: string | null;
  enableSmartRouting?: boolean | null;
}
```

**Step 2: Add model routes configuration**

Add to `backend/src/services/ai/model-router.ts`:

```typescript
const MODEL_ROUTES: Record<TaskType, ModelRoute> = {
  suggest_collections: {
    primary: "gpt-5-nano",
    fallback: ["gpt-4o-mini", "gpt-5-mini"],
    maxTokensRecommended: 8000,
    temperatureRecommended: null, // Reasoning models don't support temperature
  },
  suggest_next_game: {
    primary: "gpt-4o-mini",
    fallback: ["gpt-5-nano", "gpt-5-mini"],
    maxTokensRecommended: 4000,
    temperatureRecommended: 0.7,
  },
  generate_cover_image: {
    primary: "grok-2-image-1212",
    fallback: ["gpt-image-1.5", "dall-e-3"],
    maxTokensRecommended: 0,
    temperatureRecommended: null,
  },
};
```

**Step 3: Add reasoning model detection helper**

Add to `backend/src/services/ai/model-router.ts`:

```typescript
function isReasoningModel(model: string): boolean {
  return model.startsWith("gpt-5") || model.includes("o1") || model.includes("o3");
}
```

**Step 4: Implement selectModelForTask function**

Add to `backend/src/services/ai/model-router.ts`:

```typescript
export async function selectModelForTask(
  taskType: TaskType,
  userSettings: AiSettings,
  availableModels: string[]
): Promise<ModelSelection> {
  const route = MODEL_ROUTES[taskType];

  // Check user override based on task type
  let userOverride: string | null | undefined = null;
  if (taskType === "suggest_collections") {
    userOverride = userSettings.collectionSuggestionsModel;
  } else if (taskType === "suggest_next_game") {
    userOverride = userSettings.nextGameSuggestionsModel;
  } else if (taskType === "generate_cover_image") {
    userOverride = userSettings.coverGenerationModel;
  }

  if (userOverride) {
    return {
      model: userOverride,
      maxTokens: route.maxTokensRecommended || userSettings.maxTokens,
      temperature: route.temperatureRecommended ?? userSettings.temperature,
      isReasoningModel: isReasoningModel(userOverride),
    };
  }

  // Smart routing enabled: use recommended model
  if (userSettings.enableSmartRouting !== false) {
    // Try primary model
    if (availableModels.includes(route.primary)) {
      return {
        model: route.primary,
        maxTokens: route.maxTokensRecommended || userSettings.maxTokens,
        temperature: route.temperatureRecommended ?? userSettings.temperature,
        isReasoningModel: isReasoningModel(route.primary),
      };
    }

    // Try fallbacks
    for (const fallback of route.fallback) {
      if (availableModels.includes(fallback)) {
        return {
          model: fallback,
          maxTokens: route.maxTokensRecommended || userSettings.maxTokens,
          temperature: route.temperatureRecommended ?? userSettings.temperature,
          isReasoningModel: isReasoningModel(fallback),
        };
      }
    }
  }

  // Use user's default model
  return {
    model: userSettings.model,
    maxTokens: userSettings.maxTokens,
    temperature: userSettings.temperature,
    isReasoningModel: isReasoningModel(userSettings.model),
  };
}
```

**Step 5: Verify TypeScript compiles**

Run: `bun run typecheck`
Expected: No errors

**Step 6: Commit**

```bash
git add src/services/ai/model-router.ts
git commit -m "feat: add model router service with task-based selection"
```

---

## Task 3: Update AI Service - suggestCollections

**Files:**

- Modify: `backend/src/services/ai/service.ts`

**Step 1: Import model router**

At the top of `backend/src/services/ai/service.ts`, add import:

```typescript
import { selectModelForTask } from "./model-router";
import type { TaskType } from "./model-router";
```

**Step 2: Update suggestCollections to use router**

In `suggestCollections` function (around line 220), after getting `settings` and before the embedding check, add:

```typescript
// Get available models
const availableModels = await getAvailableModels(settings);

// Select optimal model for this task
const modelSelection = await selectModelForTask("suggest_collections", settings, availableModels);
```

**Step 3: Update generateText call to use selected model**

Replace the existing `generateText` call (around line 270) with:

```typescript
const completionParams: any = {
  model: openai(modelSelection.model),
  messages: [
    { role: "system", content: SYSTEM_PROMPTS.organizer },
    {
      role: "user",
      content: buildCollectionSuggestionsPromptWithRAG(selectedGames, library.length, theme),
    },
  ],
  maxTokens: modelSelection.maxTokens,
  responseFormat: { type: "json" },
};

// Only set temperature for non-reasoning models
if (modelSelection.temperature !== null) {
  completionParams.temperature = modelSelection.temperature;
}

const result = await generateText(completionParams);
```

**Step 4: Update activity logging to use actual model**

In the `logActivity` call (around line 300), change `settings.model` to `modelSelection.model`:

```typescript
await logActivity(
  userId,
  "suggest_collections",
  settings.provider,
  modelSelection.model, // Changed from settings.model
  usage,
  Date.now() - startTime,
  true
);
```

**Step 5: Verify TypeScript compiles**

Run: `bun run typecheck`
Expected: No errors

**Step 6: Commit**

```bash
git add src/services/ai/service.ts
git commit -m "feat: integrate model router into suggestCollections"
```

---

## Task 4: Update AI Service - suggestNextGame

**Files:**

- Modify: `backend/src/services/ai/service.ts`

**Step 1: Update suggestNextGame to use router**

In `suggestNextGame` function (around line 330), after getting `settings`, add:

```typescript
// Get available models
const availableModels = await getAvailableModels(settings);

// Select optimal model for this task
const modelSelection = await selectModelForTask("suggest_next_game", settings, availableModels);
```

**Step 2: Update generateText call**

Replace the existing `generateText` call with:

```typescript
const completionParams: any = {
  model: openai(modelSelection.model),
  messages: [
    { role: "system", content: SYSTEM_PROMPTS.curator },
    {
      role: "user",
      content: buildNextGamePromptWithRAG(selectedGames, library.length, userInput),
    },
  ],
  maxTokens: modelSelection.maxTokens,
  responseFormat: { type: "json" },
};

// Only set temperature for non-reasoning models
if (modelSelection.temperature !== null) {
  completionParams.temperature = modelSelection.temperature;
}

const result = await generateText(completionParams);
```

**Step 3: Update activity logging**

Change `settings.model` to `modelSelection.model` in the `logActivity` call:

```typescript
await logActivity(
  userId,
  "suggest_next_game",
  settings.provider,
  modelSelection.model, // Changed from settings.model
  usage,
  Date.now() - startTime,
  true
);
```

**Step 4: Verify TypeScript compiles**

Run: `bun run typecheck`
Expected: No errors

**Step 5: Commit**

```bash
git add src/services/ai/service.ts
git commit -m "feat: integrate model router into suggestNextGame"
```

---

## Task 5: Update AI Service - generateCollectionCover

**Files:**

- Modify: `backend/src/services/ai/service.ts`

**Step 1: Update generateCollectionCover to use router**

In `generateCollectionCover` function (around line 420), after getting `settings`, add:

```typescript
// Get available models
const availableModels = await getAvailableModels(settings);

// Select optimal model for this task
const modelSelection = await selectModelForTask("generate_cover_image", settings, availableModels);
```

**Step 2: Update generateImage call**

Replace the existing `generateImage` call with:

```typescript
const result = await generateImage({
  model: openai.image(modelSelection.model),
  prompt: buildCoverImagePrompt(collectionName, collectionDescription),
  n: 1,
  size: "1024x1024",
});
```

**Step 3: Update activity logging**

Change `settings.imageModel` to `modelSelection.model` in the `logActivity` call:

```typescript
await logActivity(
  userId,
  "generate_cover_image",
  settings.provider,
  modelSelection.model, // Changed from settings.imageModel
  { totalTokens: 0, promptTokens: 0, completionTokens: 0 },
  Date.now() - startTime,
  true
);
```

**Step 4: Verify TypeScript compiles**

Run: `bun run typecheck`
Expected: No errors

**Step 5: Run lint**

Run: `bun run lint`
Expected: No errors or warnings

**Step 6: Commit**

```bash
git add src/services/ai/service.ts
git commit -m "feat: integrate model router into generateCollectionCover"
```

---

## Task 6: API Endpoints for Routing Settings

**Files:**

- Modify: `backend/src/routes/ai.ts`

**Step 1: Add PUT endpoint for model routing settings**

In `backend/src/routes/ai.ts`, add new endpoint (around line 150, after existing settings endpoints):

```typescript
router.put(
  "/api/ai/settings/model-routing",
  async (req: Request, _params: Record<string, string>, user?: JWTPayload) => {
    if (!user) {
      return new Response(JSON.stringify({ error: "Unauthorized" }), {
        status: 401,
        headers: { "Content-Type": "application/json" },
      });
    }

    const body = await req.json();
    const {
      enableSmartRouting,
      collectionSuggestionsModel,
      nextGameSuggestionsModel,
      coverGenerationModel,
    } = body;

    // Update user settings
    await db
      .update(userAiSettings)
      .set({
        enableSmartRouting: enableSmartRouting !== undefined ? enableSmartRouting : undefined,
        collectionSuggestionsModel:
          collectionSuggestionsModel !== undefined ? collectionSuggestionsModel : undefined,
        nextGameSuggestionsModel:
          nextGameSuggestionsModel !== undefined ? nextGameSuggestionsModel : undefined,
        coverGenerationModel: coverGenerationModel !== undefined ? coverGenerationModel : undefined,
      })
      .where(eq(userAiSettings.userId, user.id));

    return new Response(JSON.stringify({ success: true }), {
      status: 200,
      headers: { "Content-Type": "application/json" },
    });
  },
  true
);
```

**Step 2: Verify TypeScript compiles**

Run: `bun run typecheck`
Expected: No errors

**Step 3: Run lint**

Run: `bun run lint`
Expected: No errors or warnings

**Step 4: Commit**

```bash
git add src/routes/ai.ts
git commit -m "feat: add model routing settings api endpoint"
```

---

## Task 7: Manual Verification

**No files modified - testing only**

**Step 1: Start backend server**

Run: `bun run dev`
Expected: Server starts on port 3000

**Step 2: Test collection suggestions with smart routing**

Run:

```bash
curl -X POST http://localhost:3000/api/ai/collections/suggest \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"theme": "cozy games"}'
```

Expected:

- Response contains collection suggestions
- Check backend logs for: "Using model: gpt-5-nano" or fallback model

**Step 3: Test next game suggestions**

Run:

```bash
curl -X POST http://localhost:3000/api/ai/games/suggest-next \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -H "Content-Type: application/json"
```

Expected:

- Response contains next game suggestion
- Check backend logs for: "Using model: gpt-4o-mini" or fallback model

**Step 4: Test model routing settings endpoint**

Run:

```bash
curl -X PUT http://localhost:3000/api/ai/settings/model-routing \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"enableSmartRouting": false}'
```

Expected: `{"success": true}`

**Step 5: Test with smart routing disabled**

Run collection suggestion request again (Step 2).

Expected:

- Response contains collection suggestions
- Check backend logs for: "Using model: gpt-4o" (user's default model, not gpt-5-nano)

**Step 6: Check activity logs for cost comparison**

Query database:

```sql
SELECT action, model, total_cost, created_at
FROM ai_activity_logs
WHERE user_id = 'YOUR_USER_ID'
ORDER BY created_at DESC
LIMIT 10;
```

Expected:

- Recent logs show different models per action type
- Collection suggestions use gpt-5-nano (lower cost)
- Next game suggestions use gpt-4o-mini

**Step 7: Stop server**

Press Ctrl+C to stop the backend server.

---

## Verification Checklist

- [ ] Database migration applied successfully
- [ ] New columns visible in `user_ai_settings` table
- [ ] TypeScript compiles with no errors (`bun run typecheck`)
- [ ] ESLint passes with no warnings (`bun run lint`)
- [ ] Collection suggestions use gpt-5-nano by default
- [ ] Next game suggestions use gpt-4o-mini by default
- [ ] Smart routing can be disabled via API
- [ ] Activity logs track actual model used (not settings.model)
- [ ] Reasoning models don't set temperature parameter
- [ ] Fallback works when primary model unavailable
- [ ] User overrides work per task type
- [ ] Cost savings visible in activity logs

---

## Expected Metrics

| Metric                     | Before (Phase 3) | After (Phase 4) | Improvement       |
| -------------------------- | ---------------- | --------------- | ----------------- |
| Collection suggestion cost | $0.0015          | $0.0006         | 60% reduction     |
| Next game suggestion cost  | $0.0015          | $0.0010         | 33% reduction     |
| Cover generation cost      | Variable         | Optimized       | Task-dependent    |
| Overall additional savings | -                | 20-30%          | On top of Phase 3 |

---

## Rollback Plan

If Phase 4 causes issues:

```sql
-- Disable smart routing for all users
UPDATE user_ai_settings SET enable_smart_routing = false;
```

Or revert the commits:

```bash
git log --oneline -10  # Find commit SHAs
git revert SHA1 SHA2 SHA3 SHA4 SHA5 SHA6 SHA7  # Revert all Phase 4 commits
git push
```

No breaking changes - users fall back to their default model settings.
